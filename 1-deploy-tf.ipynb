{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25b71966-724c-4c41-808b-7329e351b836",
   "metadata": {},
   "source": [
    "# Deploying with Actions and Terraform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b939235d-025f-49da-938d-bdc11391bd1d",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96311f85-78a8-4014-aa66-3b3e29df40c4",
   "metadata": {},
   "source": [
    "In the last lesson, we saw how we can use an identity provider to login to AWS from Github Actions.  In this lesson, we'll go further.\n",
    "\n",
    "Before moving on, let's think about what we would like to accomplish.  In other words, when we make a commit, what are the steps that we will need to occur to when we make some change in our code, and push up our code to github.  \n",
    "\n",
    "Try to write them down now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b94a530-574b-4a30-84b6-e803ed53a690",
   "metadata": {},
   "source": [
    "### Developing a plan\n",
    "\n",
    "Ok, so we'll want the following to occur:\n",
    "\n",
    "1. Run our tests, and if our tests pass\n",
    "2. Rebuild our image\n",
    "3. Push the image up to ECR\n",
    "4. Build the terraform infrastructure (EC2 and security groups)\n",
    "5. Pull down the docker image onto our EC2 instance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac538201-0faf-43a2-9959-3943546e6145",
   "metadata": {},
   "source": [
    "### Rebuilding the image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13379859-d04d-4fd4-957a-3eddf84263dc",
   "metadata": {},
   "source": [
    "Let's skip ahead to rebuilding our Docker image.  Ok, so if you look at our code, you'll see that the first step is to log in to our ecr repository."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d65636a-a3f3-4936-8642-f81c0dff54b2",
   "metadata": {},
   "source": [
    "Let's take some time to unpack the following."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a4c4fd-484a-4196-8bc1-0c1a26ed395a",
   "metadata": {},
   "source": [
    "```bash\n",
    "- name: Build, tag, and push image to Amazon ECR\n",
    "      env:\n",
    "        ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}\n",
    "        ECR_REPOSITORY: flask_app_sample\n",
    "        IMAGE_TAG: latest\n",
    "      working-directory: app\n",
    "      run: |\n",
    "        docker build -t $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG .\n",
    "        docker push $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d587aaa-c1db-4b89-a0d7-edb4302a5566",
   "metadata": {},
   "source": [
    "At the very top, we use `env` to declare two environmental variables into our github actions environment: `ECR_REGISTRY` and `ECR_REPOSITORY`.\n",
    "\n",
    "* `ECR_REGISTRY` Here we get the value, from the previous step where we logged into ECR for our AWS account.  Note that we labeled that step with an id, `id: login-ecr`.  And that logging in to ECR, provides a registry value in the format of `account-num.dkr.ecr.aws-region.amazonaws.com`.  This is the ECR registry where all of your repositories are stored.  You can imagine that the `login-ecr` step provides a dictionary of outputs, and that we just retrieved the registry value, and assigned it to `ECR_REGISTRY`.\n",
    "\n",
    "```python\n",
    "outputs = {'registry': 'account-num.dkr.ecr.aws-region.amazonaws.com'}\n",
    "```\n",
    "\n",
    "Ok, so then the only other component that's needed is the specific repository you would like to push the code to.  \n",
    "\n",
    "* Create a repository that matches your `ECR_REPOSITORY` value, so you can push up to there.\n",
    "\n",
    "Ok, so once we are logged into ECR, and AWS knows what repository to push to, think of the steps that we want to accomplish next.\n",
    "\n",
    "1. CD into the directory that has our docker file\n",
    "    * `working-directory: app` accomplishes this\n",
    "2. Build and push our repository to the ECR repo\n",
    "    * The lines in `run:` accomplish this.\n",
    "    * The `|` allows you to use a multiline string in github actions\n",
    "    * And then we are building and pushing our image, referencing the environmental variables set with `env`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c0f673-53c4-45f3-a66e-06dbc0e169cb",
   "metadata": {},
   "source": [
    "### Setting up terraform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a54875-a57a-4cfe-ba8e-2563fffd7382",
   "metadata": {},
   "source": [
    "Ok, so now that we have code that will build and push our docker repo, the next step is to set up code that will initialize our terraform infrastructure.\n",
    "\n",
    "Notice that we have the following steps:\n",
    "```yaml\n",
    "    - name: Setup Terraform\n",
    "      uses: hashicorp/setup-terraform@v1\n",
    "      with:\n",
    "        terraform_version: 1.0.0\n",
    "    - name: Terraform Init\n",
    "      working-directory: tf\n",
    "      run: terraform init\n",
    "\n",
    "    - name: Terraform Plan\n",
    "      working-directory: tf\n",
    "      run: terraform plan\n",
    "\n",
    "    - name: Terraform Apply\n",
    "      working-directory: tf\n",
    "      run: terraform apply -auto-approve\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37173409-4fb9-42e8-9efc-11467266717a",
   "metadata": {},
   "source": [
    "* Setup Terraform The first is a setup terraform step, that sets up the terraform cli in github actions with `uses: hashicorp/setup-terraform@v1`\n",
    "* Init, Plan, and Apply\n",
    "    * Next we move through our normal steps of calling `terraform init`, `terraform plan` and `terraform apply` to deploy our changes.\n",
    "    * **Notice**: At each step we set the `working-directory` to `tf`.  This is because, we need to be in the `tf` directory before calling each of these commands -- just like we would if we were on our laptop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d02c003-9e45-4f13-ba52-57a35bff7f09",
   "metadata": {},
   "source": [
    "* Terraform destroy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0c1b28-7b0e-48c9-845d-d7ec242f3c92",
   "metadata": {},
   "source": [
    "If something breaks in our process of deploying, we don't want to have things just lying around.  So we call `terraform destroy` if there is any failure.\n",
    "\n",
    "```yaml\n",
    "- name: Terraform Destroy\n",
    "      if: failure()\n",
    "      run: terraform destroy -auto-approve\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f448c9-7545-4981-b00a-4b1554b28635",
   "metadata": {},
   "source": [
    "> The auto-approve is because terraform will otherwise ask us to type `yes` to proceed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b9190f-1223-4ad2-99ab-4475d6d77e6c",
   "metadata": {},
   "source": [
    "### The Terraform Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa58e834-6606-4bc0-b3f5-30a387a60422",
   "metadata": {},
   "source": [
    "If you take a look at our terraform code, you can see what occurs when we boot up our ec2 instance. \n",
    "\n",
    "```bash\n",
    "user_data = <<-EOF\n",
    "  #!/bin/bash\n",
    "  export AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query \"Account\" --output text)\n",
    "  export REGION=us-east-1\n",
    "  export BACKEND_CONTAINER=flask_api\n",
    "  export REPOSITORY_NAME=flask_app_sample\n",
    "  sudo yum update -y\n",
    "  sudo yum install docker -y\n",
    "  sudo systemctl start docker\n",
    "  ...\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875bf4d7-0061-4ae2-bf7d-657232daf882",
   "metadata": {},
   "source": [
    "Ok, so the in user data we move through the normal steps of setting up docker.  Perhaps the most interesting part is the line:\n",
    "    \n",
    "```bash\n",
    "export AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query \"Account\" --output text)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa743cb-3c2f-418a-a813-de93dc9f8292",
   "metadata": {},
   "source": [
    "Here, the goal is to use the `aws` command line to get our account_id.  This way we can use that account id when pulling our image from our ECR later on, as in:\n",
    "\n",
    "```bash\n",
    "sudo docker pull $AWS_ACCOUNT_ID.dkr.ecr.$REGION.amazonaws.com/$REPOSITORY_NAME:latest\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a50f91c-5858-4b14-b653-7f85e23de784",
   "metadata": {},
   "source": [
    "Ok, so we get the account id, with the `sts get-caller-id` which returns an output in the format of:\n",
    "    \n",
    "```bash\n",
    "{\"UserId\": \"AIDACKCEVSQ6C2EXAMPLE\",\"Account\": \"123456789012\", \"Arn\": \"arn:aws:iam::123456789012:user/JohnDoe\"}\n",
    "```\n",
    "And so we query the `Account` property and store it as `AWS_ACCOUNT_ID`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6349fd-fd3e-4ab9-87aa-b0ea18372b6a",
   "metadata": {},
   "source": [
    "The rest of the above code just downloads and sets up docker.  And then further on, we move through the steps of logging our EC2 machine into our ECR to download our ECR.  And then pulling the correct repository and booting up the image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d320817-5981-446b-a59c-2dac41e2fbf4",
   "metadata": {},
   "source": [
    "```bash\n",
    "aws ecr get-login-password --region $REGION | sudo docker login --username AWS --password-stdin $AWS_ACCOUNT_ID.dkr.ecr.$REGION.amazonaws.com\n",
    "  sudo docker pull $AWS_ACCOUNT_ID.dkr.ecr.$REGION.amazonaws.com/$REPOSITORY_NAME:latest\n",
    "\n",
    "  while ! sudo docker container ls | grep -wq $BACKEND_CONTAINER; do\n",
    "    sudo docker run -d -p 80:80 --name $BACKEND_CONTAINER --platform=linux/amd64/v2 $AWS_ACCOUNT_ID.dkr.ecr.$REGION.amazonaws.com/$REPOSITORY_NAME\n",
    "    sleep 5\n",
    "  done\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4223aa7-14c1-40a7-93f9-ac47edefedf7",
   "metadata": {},
   "source": [
    "#### Making it Work for You"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440968f9-3e70-4b79-b42b-490bc5c6cf02",
   "metadata": {},
   "source": [
    "So to get this code to work for you, you'll need to go to the `tf/main.tf` file, and in that `user_data` section, you'll need to update the `region`, and `repository_name` (to match the repository name that you want to pull from in ECR).\n",
    "\n",
    "Afterwards, you should be able to push to github actions, and watch it work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db73e7a4-a10b-41c3-a6f1-d98e33acc8a4",
   "metadata": {},
   "source": [
    "<img src=\"./actions-apply.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f9d2d6-601d-40ad-ac2c-9d7882f1301c",
   "metadata": {},
   "source": [
    "And then you can go to your aws web console to find the EC2 that you just launched, as well as the domain name to connect to.\n",
    "\n",
    "Go to `http://domain_name` (not `https`) and you should see your website."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbc2087-144c-4fe6-a3af-3d4c65fa876f",
   "metadata": {},
   "source": [
    "> <img src=\"./working-container.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f574eae-452e-43b0-a9a3-8c2ca65f0b01",
   "metadata": {},
   "source": [
    "### What's next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226bc86d-b4cf-40da-b6e4-260817a14c8a",
   "metadata": {},
   "source": [
    "Ok, so we did develop a github actions pipeline that will build our docker image, push it to ecr, setup our terraform infrastructure, and pull down our images, and boot up a container.\n",
    "\n",
    "However, in production we would have a problem in that each time we push to github actions, a new ec2 machine would be created.  And there's not really to run `terraform destroy` to delete or update our infrastructure, after our github actions pipeline has completed.\n",
    "\n",
    "The problem is that to reference the infrastructure that was just created, terraform needs the `terraform.tfstate` file -- and that is on your github actions environment.  So the next step would be to store our `terraform.tfstate` file in an S3 bucket, and use that file when calling our terraform commands.  \n",
    "\n",
    "> Check it out: You can see how to do this in the [following blog post](https://faun.pub/deploy-to-aws-ec2-using-terraform-and-github-actions-ci-cd-bfcb60b3cc1e).\n",
    "\n",
    "* One other issue\n",
    "\n",
    "Once we do that, we would also want to make sure our `user_data` script that pulls down our docker image from ECR is run each time we update our web application code, even if we do not update our terraform code.  To make sure that script is run, one trick is to place a timestamp in the `user_data` code, this way we ensure our terraform code changes, and thus our user_data code is re-run each time.\n",
    "\n",
    "```bash\n",
    "locals {\n",
    "  timestamp = timestamp()\n",
    "}\n",
    "\n",
    "resource \"aws_instance\" \"example\" {\n",
    "  ami           = \"ami-123456\"\n",
    "  instance_type = \"t2.micro\"\n",
    "\n",
    "  user_data = <<-EOF\n",
    "                #!/bin/bash\n",
    "                echo \"Deployed at: ${local.timestamp}\"\n",
    "                ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4aa0d6-dffc-4fb4-b57c-8c2b39e13d42",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "[Next Steps - CI CD and TF](https://faun.pub/deploy-to-aws-ec2-using-terraform-and-github-actions-ci-cd-bfcb60b3cc1e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
